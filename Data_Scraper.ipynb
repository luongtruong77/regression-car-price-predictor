{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adverse-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time, os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "print('Libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "consolidated-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver_path = \"C:\\Metis_Bootcamp\\Regression_project\\chromedriver.exe\"\n",
    "\n",
    "#This header is to bypass CAPCHA\n",
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "#Dataframe headers\n",
    "df_headers = [\"Name\", \"Mileage\", \"Address\", \"Rating\", \"Fuel Type\", \"City MPG\", \n",
    "               \"Highway MPG\", \"Drivetrain\", \"Engine\", \"Exterior Color\", \"Interior Color\", \n",
    "              \"Transmission\", \"Entertainment\", \"Safety\", \"Price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "negative-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the function to get all urls of 1 page\n",
    "\n",
    "def get_one_page_urls(page_link):\n",
    "    \n",
    "    \"\"\"\n",
    "    Functionality: Get all item's urls that are listed in one page\n",
    "    Parameter: the link to our desired page\n",
    "    Return: The list of all item's urls that are listed in the page\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(page_link, headers=header)\n",
    "\n",
    "    data = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    \n",
    "    one_page_urls = []\n",
    "    \n",
    "    #Loop through all the items inside the listing container of the html page\n",
    "    #and find the link (href) inside the anchor tag and append it \n",
    "    #into the list. Since the link is shortened, we have to put it\n",
    "    #after the main url (https://www.cars.com/)\n",
    "    \n",
    "    for i in range(len(soup.find_all(class_=\"shop-srp-listings__listing-container\"))):\n",
    "        url = soup.find_all(class_=\"shop-srp-listings__listing-container\")[i].find('a')['href']\n",
    "        one_page_urls.append(f\"https://www.cars.com/{url}\")\n",
    "    \n",
    "    return one_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "growing-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(start_page, num_of_pages, radius, zipcode):\n",
    "    \n",
    "    \"\"\"\n",
    "    Functionality: Get all wanted urls based on the specific area.\n",
    "    Parameters: the staring page, the number of pages we want to loop through, \n",
    "    the radius from the center of the zipcode, the area's zipcode.\n",
    "    Return: The list of all item's urls.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    all_urls_list = []\n",
    "    \n",
    "    for i in range(num_of_pages):\n",
    "        \n",
    "        #Define the root url and pass in 3 parameters: start_page, num_of_pages, zipcode\n",
    "        root_url = \"https://www.cars.com/for-sale/searchresults.action/?page={}&perPage=100&rd={}&searchSource=GN_BREADCRUMB&sort=relevance&zc={}\".format(i+start_page, radius, zipcode)\n",
    "        \n",
    "        #Append all urls to the list\n",
    "        all_urls_list.append(get_one_page_urls(root_url))\n",
    "    \n",
    "    #Depends on how many pages we loop through, we will get the list with that \n",
    "    #many sublists inside of all_urls_list. We want the function to return\n",
    "    #one single list, so we use list comprehension.\n",
    "    all_urls = [item for sub_list in all_urls_list for item in sub_list]\n",
    "    \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lucky-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_car_features(url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Functionality: Get car's features based on the car's url\n",
    "    Parameters: the url that is linked to the item.\n",
    "    Return: The list of dictionaries where the keys are the features and the values are the features' values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url, headers=header)\n",
    "\n",
    "    data = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    \n",
    "    #Navigate to the basic features containers and grab all of them, put them in a list\n",
    "    basic_features = soup.find_all('li', class_='vdp-details-basics__item')\n",
    "    basic_feature_list = []\n",
    "    \n",
    "    for i in range(len(basic_features)):\n",
    "        basic_feature_list.append(soup.find_all('li', class_='vdp-details-basics__item')[i].text.strip().split(':'))\n",
    "    \n",
    "    #Navigate to the extra features container and grab all of them, put them in a list\n",
    "    extra_features = soup.find_all(class_='details-feature-list details-feature-list--normalized-features')\n",
    "    extra_feature_list = []\n",
    "    for i in range(len(extra_features)):\n",
    "        extra_feature_list.append(soup.find_all(class_='details-feature-list details-feature-list--normalized-features')[i].text.strip().split('\\n\\n'))\n",
    "    \n",
    "    #Sort through the structure of the page to and grab features\n",
    "    name = soup.find('h1', class_='cui-heading-2--secondary vehicle-info__title').text\n",
    "    mileage = soup.find(class_='vdp-cap-price__mileage--mobile vehicle-info__mileage mileage_margin').text.split(\" \")[0]\n",
    "    address = soup.find(class_='get-directions-link seller-details-location__text').text.strip()\n",
    "    rating = soup.find(class_='rating__link rating__link--has-reviews').text.split('(')[1].split(')')[0]\n",
    "    \n",
    "    #Define default values for these features in case any of them missing from specific item's page\n",
    "    #the default value will be -1\n",
    "    fuel_type = -1\n",
    "    city_MPG = -1\n",
    "    highway_MPG = -1\n",
    "    drivetrain = -1\n",
    "    engine = -1\n",
    "    exterior_color = -1\n",
    "    interior_color = -1\n",
    "    transmission = -1\n",
    "    entertainment = -1\n",
    "    safety = -1\n",
    "    price = soup.find(class_='vehicle-info__price-display vehicle-info__price-display--dealer cui-heading-2').text\n",
    "    \n",
    "    \n",
    "    #Loop through the basic feature list and assign value for each of them\n",
    "    for basic in basic_feature_list:\n",
    "        if basic[0] == \"Fuel Type\":\n",
    "            fuel_type = basic[1]\n",
    "        if basic[0] == 'City MPG':\n",
    "            city_MPG = basic[1].split(\" \")[1]\n",
    "        if basic[0] == 'Highway MPG':\n",
    "            highway_MPG = basic[1].split(\" \")[1]\n",
    "        if basic[0] == 'Drivetrain':\n",
    "            drivetrain = basic[1]\n",
    "        if basic[0] == 'Engine':\n",
    "            engine = basic[1]\n",
    "        if basic[0] == 'Exterior Color':\n",
    "            exterior_color = basic[1]\n",
    "        if basic[0] == 'Interior Color':\n",
    "            interior_color = basic[1]\n",
    "        if basic[0] == 'Transmission':\n",
    "            transmission = basic[1]\n",
    "    \n",
    "    #Same for extra feature\n",
    "    for extra in extra_feature_list:\n",
    "        if extra[0] == \"Entertainment\":\n",
    "            entertainment = extra[1].split('\\n')\n",
    "        if extra[0] == \"Safety\":\n",
    "            safety = extra[1].split('\\n')\n",
    "            \n",
    "    #Put all features in a dictionary\n",
    "    car_dict = dict(zip(df_headers, [name,\n",
    "                             mileage,\n",
    "                             address,\n",
    "                             rating,\n",
    "                             fuel_type,\n",
    "                             city_MPG,\n",
    "                             highway_MPG,\n",
    "                             drivetrain,\n",
    "                             engine,\n",
    "                             exterior_color,\n",
    "                             interior_color,\n",
    "                             transmission,\n",
    "                             entertainment,\n",
    "                             safety,\n",
    "                             price]))\n",
    "     \n",
    "    #Return the list of dictionaries    \n",
    "    return cars_list.append(car_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "metropolitan-barbados",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 1 to page 10\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 11 to page 20\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 21 to page 30\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 31 to page 40\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 41 to page 50\n"
     ]
    }
   ],
   "source": [
    "start_page = 1\n",
    "\n",
    "zipcode = 98409 #Tacoma Area\n",
    "\n",
    "for i in range(5): \n",
    "    \n",
    "    list_of_url = get_all_urls(start_page=start_page, num_of_pages=10, radius=50, zipcode=98409)\n",
    "    \n",
    "    cars_list = []\n",
    "\n",
    "    for url in list_of_url:\n",
    "        try:\n",
    "            get_car_features(url)\n",
    "        except:\n",
    "            print('..')\n",
    "\n",
    "    df = pd.DataFrame(cars_list)\n",
    "    \n",
    "    df.to_csv('data/df_1000_{}_p{}_p{}.csv'.format(zipcode, start_page, start_page+9))\n",
    "    \n",
    "    print(\"Done for page {} to page {}\".format(start_page, start_page+9))\n",
    "    \n",
    "    start_page += 10\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "choice-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_url = get_all_urls(start_page=11, num_of_pages=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-founder",
   "metadata": {},
   "source": [
    "list_of_url_1 = open('data/list_of_url_1.obj', 'wb')\n",
    "pickle.dump(list_of_url, list_of_url_1)\n",
    "list_of_url_1.close()\n",
    "\n",
    "example = open('data/list_of_url_1.obj', 'rb')\n",
    "exampleObj = pickle.load(example)\n",
    "example.close()\n",
    "print(exampleObj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
