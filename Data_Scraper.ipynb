{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adverse-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time, os\n",
    "\n",
    "chrome_driver_path = \"C:\\Metis_Bootcamp\\Regression_project\\chromedriver.exe\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "consolidated-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "df_headers = [\"Name\", \"Mileage\", \"Address\", \"Rating\", \"Fuel Type\", \"City MPG\", \n",
    "               \"Highway MPG\", \"Drivetrain\", \"Engine\", \"Exterior Color\", \"Interior Color\", \n",
    "              \"Transmission\", \"Entertainment\", \"Safety\", \"Price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "negative-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_page_urls(page_link):\n",
    "    \n",
    "    response = requests.get(page_link, headers=header)\n",
    "\n",
    "    data = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    \n",
    "    one_page_urls = []\n",
    "    \n",
    "    for i in range(len(soup.find_all(class_=\"shop-srp-listings__listing-container\"))):\n",
    "        url = soup.find_all(class_=\"shop-srp-listings__listing-container\")[i].find('a')['href']\n",
    "        one_page_urls.append(f\"https://www.cars.com/{url}\")\n",
    "    \n",
    "    return one_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "growing-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls(start_page, num_of_pages, radius, zipcode):\n",
    "    \n",
    "    all_urls_list = []\n",
    "    \n",
    "    for i in range(num_of_pages):\n",
    "        \n",
    "        root_url = \"https://www.cars.com/for-sale/searchresults.action/?page={}&perPage=100&rd={}&searchSource=GN_BREADCRUMB&sort=relevance&zc={}\".format(i+start_page, radius, zipcode)\n",
    "        \n",
    "        all_urls_list.append(get_one_page_urls(root_url))\n",
    "    \n",
    "    all_urls = [item for sub_list in all_urls_list for item in sub_list]\n",
    "    \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lucky-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_car_features(url):\n",
    "    \n",
    "    response = requests.get(url, headers=header)\n",
    "\n",
    "    data = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    \n",
    "    basic_features = soup.find_all('li', class_='vdp-details-basics__item')\n",
    "    basic_feature_list = []\n",
    "    for i in range(len(basic_features)):\n",
    "        basic_feature_list.append(soup.find_all('li', class_='vdp-details-basics__item')[i].text.strip().split(':'))\n",
    "\n",
    "    extra_features = soup.find_all(class_='details-feature-list details-feature-list--normalized-features')\n",
    "    extra_feature_list = []\n",
    "    for i in range(len(extra_features)):\n",
    "        extra_feature_list.append(soup.find_all(class_='details-feature-list details-feature-list--normalized-features')[i].text.strip().split('\\n\\n'))\n",
    "        \n",
    "    name = soup.find('h1', class_='cui-heading-2--secondary vehicle-info__title').text\n",
    "    mileage = soup.find(class_='vdp-cap-price__mileage--mobile vehicle-info__mileage mileage_margin').text.split(\" \")[0]\n",
    "    address = soup.find(class_='get-directions-link seller-details-location__text').text.strip()\n",
    "    rating = soup.find(class_='rating__link rating__link--has-reviews').text.split('(')[1].split(')')[0]\n",
    "    fuel_type = -1\n",
    "    city_MPG = -1\n",
    "    highway_MPG = -1\n",
    "    drivetrain = -1\n",
    "    engine = -1\n",
    "    exterior_color = -1\n",
    "    interior_color = -1\n",
    "    transmission = -1\n",
    "    entertainment = -1\n",
    "    safety = -1\n",
    "    price = soup.find(class_='vehicle-info__price-display vehicle-info__price-display--dealer cui-heading-2').text\n",
    "\n",
    "    for basic in basic_feature_list:\n",
    "        if basic[0] == \"Fuel Type\":\n",
    "            fuel_type = basic[1]\n",
    "        if basic[0] == 'City MPG':\n",
    "            city_MPG = basic[1].split(\" \")[1]\n",
    "        if basic[0] == 'Highway MPG':\n",
    "            highway_MPG = basic[1].split(\" \")[1]\n",
    "        if basic[0] == 'Drivetrain':\n",
    "            drivetrain = basic[1]\n",
    "        if basic[0] == 'Engine':\n",
    "            engine = basic[1]\n",
    "        if basic[0] == 'Exterior Color':\n",
    "            exterior_color = basic[1]\n",
    "        if basic[0] == 'Interior Color':\n",
    "            interior_color = basic[1]\n",
    "        if basic[0] == 'Transmission':\n",
    "            transmission = basic[1]\n",
    "\n",
    "    for extra in extra_feature_list:\n",
    "        if extra[0] == \"Entertainment\":\n",
    "            entertainment = extra[1].split('\\n')\n",
    "        if extra[0] == \"Safety\":\n",
    "            safety = extra[1].split('\\n')\n",
    "            \n",
    "    car_dict = dict(zip(df_headers, [name,\n",
    "                             mileage,\n",
    "                             address,\n",
    "                             rating,\n",
    "                             fuel_type,\n",
    "                             city_MPG,\n",
    "                             highway_MPG,\n",
    "                             drivetrain,\n",
    "                             engine,\n",
    "                             exterior_color,\n",
    "                             interior_color,\n",
    "                             transmission,\n",
    "                             entertainment,\n",
    "                             safety,\n",
    "                             price]))\n",
    " \n",
    "    return cars_list.append(car_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-barbados",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 1 to page 10\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "Done for page 11 to page 20\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "start_page = 1\n",
    "\n",
    "zipcode = 98409 #Tacoma Area\n",
    "\n",
    "for i in range(5): \n",
    "    \n",
    "    list_of_url = get_all_urls(start_page=start_page, num_of_pages=10, radius=50, zipcode=98409)\n",
    "    \n",
    "    cars_list = []\n",
    "\n",
    "    for url in list_of_url:\n",
    "        try:\n",
    "            get_car_features(url)\n",
    "        except:\n",
    "            print('..')\n",
    "\n",
    "    df = pd.DataFrame(cars_list)\n",
    "    \n",
    "    df.to_csv('data/df_1000_{}_p{}_p{}.csv'.format(zipcode, start_page, start_page+9))\n",
    "    \n",
    "    print(\"Done for page {} to page {}\".format(start_page, start_page+9))\n",
    "    \n",
    "    start_page += 10\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "choice-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_url = get_all_urls(start_page=11, num_of_pages=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-founder",
   "metadata": {},
   "source": [
    "list_of_url_1 = open('data/list_of_url_1.obj', 'wb')\n",
    "pickle.dump(list_of_url, list_of_url_1)\n",
    "list_of_url_1.close()\n",
    "\n",
    "example = open('data/list_of_url_1.obj', 'rb')\n",
    "exampleObj = pickle.load(example)\n",
    "example.close()\n",
    "print(exampleObj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
